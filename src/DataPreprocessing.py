from numpy import sign
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
import os


# Táº¡o Spark session
def create_spark():
    spark = SparkSession.builder \
        .appName("AccidentsLogisticRegression") \
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
        .getOrCreate()
    return spark

# Load dá»¯ liá»‡u
def load_data(spark, path):
    df = spark.read.csv(path, header=True, inferSchema=True)
    print("ğŸ“¥ ÄÃ£ load dá»¯ liá»‡u:", df.count(), "dÃ²ng")
    return df

# LÃ m sáº¡ch dá»¯ liá»‡u
# 3. LÃ m sáº¡ch dá»¯ liá»‡u (Clean Data)
def clean_data(df):
    print("ğŸ§¹ Äang lÃ m sáº¡ch dá»¯ liá»‡u...")

    # Chá»n cÃ¡c cá»™t cáº§n thiáº¿t cho MainModel
    # LÆ¯U Ã: Pháº£i giá»¯ láº¡i cÃ¡c cá»™t mÃ  MainModel.py cáº§n dÃ¹ng
    required_cols = [
        "Severity", "Start_Time", "Start_Lat", "Start_Lng",
        "Distance(mi)", "Temperature(F)", "Humidity(%)",
        "Pressure(in)", "Visibility(mi)", "Wind_Speed(mph)",
        "Weather_Condition", "Sunrise_Sunset"
    ]

    # Chá»‰ láº¥y cÃ¡c cá»™t tá»“n táº¡i trong file
    selected_cols = [c for c in required_cols if c in df.columns]
    df = df.select(selected_cols)

    # Loáº¡i bá» dÃ²ng thiáº¿u dá»¯ liá»‡u (Null) á»Ÿ cÃ¡c cá»™t quan trá»ng
    df = df.dropna(subset=selected_cols)

    # Loáº¡i bá» cÃ¡c giÃ¡ trá»‹ trÃ¹ng láº·p
    df = df.dropDuplicates()

    print(f"   Sá»‘ dÃ²ng sau khi lÃ m sáº¡ch: {df.count()}")
    return df


def feature_engineering(df):
    print("âš™ï¸  Äang táº¡o feature thá»i gian (Hour, Weekday, Month)...")

    if "Start_Time" in df.columns:
        df = df.withColumn("Hour", hour("Start_Time")) \
            .withColumn("Weekday", dayofweek("Start_Time")) \
            .withColumn("Month", month("Start_Time"))

    return df
# 5. MÃ£ hÃ³a dá»¯ liá»‡u chá»¯ (HÃ m cá»§a báº¡n)
def encode_categorical_cols(df, cat_cols):
    """
    PhiÃªn báº£n AN TOÃ€N: Tá»± Ä‘á»™ng bá» qua cÃ¡c cá»™t khÃ´ng tÃ¬m tháº¥y.
    """
    print(f"\n--- Äang mÃ£ hÃ³a dá»¯ liá»‡u (Label Encoding) ---")

    # Lá»c danh sÃ¡ch: Chá»‰ xá»­ lÃ½ nhá»¯ng cá»™t THá»°C Sá»° CÃ“ trong df
    valid_cat_cols = [c for c in cat_cols if c in df.columns]

    if not valid_cat_cols:
        print("âš ï¸ KhÃ´ng cÃ³ cá»™t string nÃ o há»£p lá»‡ Ä‘á»ƒ mÃ£ hÃ³a.")
        return df, []

    indexers = []
    new_cat_cols = []

    for col_name in valid_cat_cols:
        new_col_name = col_name + "_idx"
        new_cat_cols.append(new_col_name)

        # handleInvalid="keep": Giá»¯ láº¡i giÃ¡ trá»‹ láº¡ thay vÃ¬ bÃ¡o lá»—i
        indexer = StringIndexer(inputCol=col_name, outputCol=new_col_name, handleInvalid="keep")
        indexers.append(indexer)

    try:
        pipeline = Pipeline(stages=indexers)
        model = pipeline.fit(df)
        encoded_df = model.transform(df)
        print(f"âœ… ÄÃ£ mÃ£ hÃ³a thÃ nh cÃ´ng: {valid_cat_cols} -> {new_cat_cols}")
        return encoded_df, new_cat_cols

    except Exception as e:
        print(f"âŒ Lá»—i khi cháº¡y StringIndexer: {e}")
        return df, []
# Save file output - DÃ¹ng Pandas thay vÃ¬ Spark CSV
def save_output(df):
    output_folder = "../data"
    output_path = os.path.join(output_folder, "data_final_processed")
    print("Do dá»¯ liá»‡u quÃ¡ lá»›n nÃªn pháº£i luÆ° báº±ng Spark .Xin lá»—i vÃ¬ lÃ m máº¥t thá»i gian")
    print(f"ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u báº±ng Spark vÃ o thÆ° má»¥c: {output_path}")
    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    try:
        df.write.mode("overwrite").option("header", "true").csv(output_path)

        print(f"âœ… LÆ¯U THÃ€NH CÃ”NG!")
        print(f"   LÆ°u Ã½: Spark lÆ°u thÃ nh má»™t THÆ¯ Má»¤C tÃªn lÃ  '{os.path.basename(output_path)}'.")
        print(f"   BÃªn trong Ä‘Ã³ chá»©a cÃ¡c file .csv (part-0000...). ÄÃ¢y lÃ  tÃ­nh nÄƒng cá»§a Spark.")

    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file: {e}")
# Pipeline chÃ­nh
def preprocess(path):
    spark = create_spark()

    # 1. Load dá»¯ liá»‡u
    df = load_data(spark, path)

    # 2. LÃ m sáº¡ch (Thay vÃ¬ feature_area/feature_time gÃ¢y lá»—i)
    df = clean_data(df)

    # 3. Táº¡o feature thá»i gian
    df = feature_engineering(df)

    # 4. MÃ£ hÃ³a Weather vÃ  Sunrise
    cat_cols = ["Weather_Condition", "Sunrise_Sunset"]
    df, encoded_cols = encode_categorical_cols(df, cat_cols)

    # In káº¿t quáº£ kiá»ƒm tra
    print("Pipeline hoÃ n táº¥t.")
    # LÆ°u file
    save_output(df)
    print(f"Tá»•ng sá»‘ dÃ²ng dá»¯ liá»‡u sáº¡ch: {df.count()}")
    # Tráº£ vá» DataFrame vÃ  danh sÃ¡ch cá»™t mÃ£ hÃ³a Ä‘á»ƒ MainModel dÃ¹ng
    return df, encoded_cols

if __name__ == "__main__":
    # ÄÆ°á»ng dáº«n file
    file_path = "../data/US_Accidents_March23.csv"

    if os.path.exists(file_path):
        df_result, cols_result = preprocess(file_path)
        df_result.show(5)
    else:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file táº¡i: {file_path}")