from pyspark.sql.functions import *
import pandas as pd
import os


def descriptive_statistics(df):
    """
    In th·ªëng k√™ m√¥ t·∫£ s∆° b·ªô ra m√†n h√¨nh console.
    """
    print("\n" + "=" * 70)
    print("DESCRIPTIVE STATISTICS (PREVIEW)")
    print("=" * 70)

    # 1. Ch·ªçn v√†i c·ªôt quan tr·ªçng ƒë·ªÉ hi·ªÉn th·ªã demo (tr√°nh in qu√° nhi·ªÅu lo·∫°n m·∫Øt)
    demo_cols = ["Severity", "Temperature(F)", "Visibility(mi)", "Distance(mi)"]
    # Ch·ªâ ch·ªçn nh·ªØng c·ªôt th·ª±c s·ª± c√≥ trong df
    existing_cols = [c for c in demo_cols if c in df.columns]

    if existing_cols:
        df.select(existing_cols).describe().show()
    else:
        # N·∫øu kh√¥ng t√¨m th·∫•y c·ªôt quen thu·ªôc, hi·ªÉn th·ªã 5 c·ªôt ƒë·∫ßu ti√™n
        df.select(df.columns[:5]).describe().show()

    # 2. Ph√¢n ph·ªëi Severity (M·ª©c ƒë·ªô nghi√™m tr·ªçng)
    print("\nSEVERITY DISTRIBUTION:")
    if "Severity" in df.columns:
        df.groupBy("Severity").count().orderBy("Severity").show()
    elif "label" in df.columns:
        print("(Kh√¥ng th·∫•y c·ªôt Severity, hi·ªÉn th·ªã c·ªôt label)")
        df.groupBy("label").count().orderBy("label").show()

    print("=" * 70)


def save_descriptive_to_file(df):
    """
    T√≠nh to√°n chi ti·∫øt v√† l∆∞u file CSV.
    ƒê√£ t·ªëi ∆∞u h√≥a ƒë·ªÉ ch·∫°y nhanh h∆°n tr√™n Spark.
    """
    # ===== PATH =====
    SRC_DIR = os.path.dirname(os.path.abspath(__file__))
    BASE_DIR = os.path.dirname(SRC_DIR)
    RESULT_DIR = os.path.join(BASE_DIR, "results")
    os.makedirs(RESULT_DIR, exist_ok=True)

    # ===== 1. X√ÅC ƒê·ªäNH C·ªòT S·ªê =====
    # L·∫•y t·∫•t c·∫£ c·ªôt s·ªë tr·ª´ ID v√† c√°c c·ªôt ƒë√£ m√£ h√≥a (_idx)
    numeric_cols = [
        c for c, t in df.dtypes
        if t in ("int", "double", "float", "long")
           and c not in ["ID", "Severity", "label"]
           and not c.endswith("_idx")
    ]

    print(f"üìä ƒêang t√≠nh to√°n th·ªëng k√™ cho {len(numeric_cols)} c·ªôt s·ªë...")
    summary = []

    for c in numeric_cols:
        try:
            # --- T·ªêI ∆ØU H√ìA: T√≠nh Mean, Min, Max, Std... trong 1 l·ªánh Spark duy nh·∫•t ---
            stats = df.select(
                count(c).alias("Count"),
                mean(c).alias("Mean"),
                stddev(c).alias("Std"),
                min(c).alias("Min"),
                max(c).alias("Max"),
                skewness(c).alias("Skewness"),
                kurtosis(c).alias("Kurtosis")
            ).first()

            # T√≠nh Quantile (ri√™ng th·∫±ng n√†y ph·∫£i t√≠nh ri√™ng)
            q1, median, q3 = df.approxQuantile(c, [0.25, 0.5, 0.75], 0.01)

            row = {
                "Feature": c,
                "Count": stats["Count"],
                "Mean": stats["Mean"],
                "Median": median,
                "Std": stats["Std"],
                # "Variance": stats["Std"]**2 if stats["Std"] else 0, # C√≥ th·ªÉ b·ªè qua Variance n·∫øu kh√¥ng c·∫ßn thi·∫øt
                "Min": stats["Min"],
                "Max": stats["Max"],
                "Q1": q1,
                "Q3": q3,
                "Skewness": stats["Skewness"],
                "Kurtosis": stats["Kurtosis"]
            }
            summary.append(row)
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh to√°n c·ªôt {c}: {e}")

    # ===== 2. L∆ØU FILE TH·ªêNG K√ä CHI TI·∫æT =====
    if summary:
        pd.DataFrame(summary).to_csv(
            os.path.join(RESULT_DIR, "descriptive_statistics.csv"),
            index=False
        )

    # ===== 3. L∆ØU FILE PH√ÇN PH·ªêI SEVERITY =====
    target_col = "Severity" if "Severity" in df.columns else "label"

    if target_col in df.columns:
        dist_df = df.groupBy(target_col).count().orderBy(target_col)
        # Chuy·ªÉn sang Pandas ƒë·ªÉ l∆∞u CSV
        dist_df.toPandas().to_csv(
            os.path.join(RESULT_DIR, "severity_distribution.csv"),
            index=False
        )

    print("‚úÖ ƒê√£ l∆∞u th·ªëng k√™ m√¥ t·∫£:")
    print(f"   - {os.path.join(RESULT_DIR, 'descriptive_statistics.csv')}")
    print(f"   - {os.path.join(RESULT_DIR, 'severity_distribution.csv')}")